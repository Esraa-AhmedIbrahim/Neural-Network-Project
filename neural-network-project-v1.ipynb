{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1624449,"sourceType":"datasetVersion","datasetId":960167},{"sourceId":10132305,"sourceType":"datasetVersion","datasetId":6253389},{"sourceId":10296829,"sourceType":"datasetVersion","datasetId":6373163},{"sourceId":10302411,"sourceType":"datasetVersion","datasetId":6376943}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python -m spacy download en_core_web_md\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-26T09:23:08.123466Z","iopub.execute_input":"2024-12-26T09:23:08.123723Z","iopub.status.idle":"2024-12-26T09:23:15.83921Z","shell.execute_reply.started":"2024-12-26T09:23:08.1237Z","shell.execute_reply":"2024-12-26T09:23:15.837995Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pre-processing","metadata":{}},{"cell_type":"code","source":"import os\nimport logging\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport spacy\nimport re\nimport nltk\nimport string\nimport unicodedata\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport emoji\nfrom bs4 import BeautifulSoup\nfrom tensorflow.keras.layers import Embedding, Dense, Dropout, Input, Conv1D, GlobalMaxPooling1D, Flatten\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport concurrent.futures\n# Check GPU availability and set memory growth\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Using GPU: {gpus}\")\n    except RuntimeError as e:\n        print(e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T09:23:15.841514Z","iopub.execute_input":"2024-12-26T09:23:15.841886Z","iopub.status.idle":"2024-12-26T09:23:15.851965Z","shell.execute_reply.started":"2024-12-26T09:23:15.841847Z","shell.execute_reply":"2024-12-26T09:23:15.85104Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pre-processing","metadata":{}},{"cell_type":"code","source":"logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Disable XLA optimization\nos.environ['XLA_FLAGS'] = '--xla_cpu_multi_thread_eigen=false'\n\n# Load SpaCy English model\nnlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n\n# Download NLTK resources\nnltk.download('stopwords', quiet=True)\nnltk.download('punkt', quiet=True)\n\n# Initialize stopwords and stemmer\nstop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer(\"english\")\n\n# Contraction mapping\nCONTRACTION_MAPPING = {\n    \"won't\": \"will not\", \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\", \n    \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\", \n    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"hadn't\": \"had not\", \"he's\": \"he is\", \n    \"she's\": \"she is\", \"it's\": \"it is\", \"that's\": \"that is\"\n}\n\n# Text cleaning functions\ndef normalize_elongations(text):\n    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n\ndef convert_to_ascii(input_str):\n    nfkd_form = unicodedata.normalize('NFKD', input_str)\n    return ''.join([c for c in nfkd_form if unicodedata.category(c) != 'Mn']).lower()\n\ndef clean_contractions(text, mapping):\n    for word, replacement in mapping.items():\n        text = text.replace(word, replacement)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub(r'http\\S+|www\\S+|@\\S+|[^a-zA-Z\\'\\-\\s]', '', text)  # keep apostrophes and dashes\n    return text\n\ndef clean_text(text):\n    if text is None:\n        return \"\"\n    try:\n        text = text.encode('latin1').decode('utf-8')\n    except (UnicodeEncodeError, UnicodeDecodeError):\n        logging.warning(f\"Unicode error during decoding of: {text}\")\n        return \"\"\n    text = emoji.demojize(text)\n    text = BeautifulSoup(text, 'lxml').get_text()\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = convert_to_ascii(text)\n    text = normalize_elongations(text)\n    text = clean_contractions(text, CONTRACTION_MAPPING)\n    text = re.sub(r'[\" \"]+', \" \", text)\n    doc = nlp(text)\n    text = [stemmer.stem(token.text) for token in doc if token.text not in stop_words and not token.is_punct]\n    return ' '.join(text).strip()\n\n# Parallel text cleaning\ndef parallel_clean_text(X_data, num_workers=os.cpu_count()):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        return list(executor.map(clean_text, X_data))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T09:23:24.144484Z","iopub.execute_input":"2024-12-26T09:23:24.144801Z","iopub.status.idle":"2024-12-26T09:23:25.712884Z","shell.execute_reply.started":"2024-12-26T09:23:24.144775Z","shell.execute_reply":"2024-12-26T09:23:25.71221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and preprocess dataset\ntrain_data = pd.read_csv('/kaggle/input/neural-network-dataset/Forum Discussion Categorization/train.csv')\nX = train_data['Discussion'].fillna('').astype(str)\ny = train_data['Category']\n\nX_train_cleaned = parallel_clean_text(X)\n\n# Filter out empty texts\nnon_empty_mask = [bool(text.strip()) for text in X_train_cleaned]\nfiltered_texts = [text for text, keep in zip(X_train_cleaned, non_empty_mask) if keep]\nfiltered_labels = y[non_empty_mask]\n\nX_train, X_val, y_train, y_val = train_test_split(filtered_texts, filtered_labels, test_size=0.2, random_state=42)\n\n# Encode labels\nlabel_mapping = {'Politics': 0, 'Sports': 1, 'Media': 2, 'Market & Economy': 3, 'STEM': 4}\ny_train_encoded = y_train.map(label_mapping)\ny_val_encoded = y_val.map(label_mapping)\n\n# Convert labels to TensorFlow tensors\ny_train_encoded_tensor = tf.convert_to_tensor(y_train_encoded, dtype=tf.int32)\ny_val_encoded_tensor = tf.convert_to_tensor(y_val_encoded, dtype=tf.int32)\n\n\n\n# Tokenizer setup\ntokenizer = nltk.word_tokenize\n\n# Build vocabulary from training data\nvocab = {}\nfor text in X_train:\n    for word in tokenizer(text):\n        if word not in vocab:\n            vocab[word] = len(vocab) + 1  # Reserve 0 for padding\n\n# Convert text to sequences\ndef text_to_sequence(text):\n    return [vocab.get(word, 0) for word in tokenizer(text)]\n\nX_train_seq = [text_to_sequence(text) for text in X_train]\nX_val_seq = [text_to_sequence(text) for text in X_val]\n\n# Pad sequences\nmax_len = 100\nX_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\nX_val_padded = pad_sequences(X_val_seq, maxlen=max_len, padding='post', truncating='post')\n\n# One-hot encode labels\ny_train_encoded_categorical = to_categorical(y_train_encoded, num_classes=len(label_mapping))\ny_val_encoded_categorical = to_categorical(y_val_encoded, num_classes=len(label_mapping))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T09:23:27.44365Z","iopub.execute_input":"2024-12-26T09:23:27.444013Z","iopub.status.idle":"2024-12-26T09:26:43.455209Z","shell.execute_reply.started":"2024-12-26T09:23:27.443983Z","shell.execute_reply":"2024-12-26T09:26:43.454137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DNN","metadata":{}},{"cell_type":"code","source":"# Define DNN model\ndef build_dnn_model(vocab_size, embedding_dim, max_len, label_count):\n    input_layer = Input(shape=(max_len,))\n    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)(input_layer)\n    dropout_layer = Dropout(0.5)(embedding_layer)\n    pooled_output = GlobalMaxPooling1D()(dropout_layer)\n\n    dense_layer_1 = Dense(128, activation='relu', kernel_regularizer=l2(0.0001))(pooled_output)\n    dropout_layer_2 = Dropout(0.5)(dense_layer_1)\n    dense_layer_2 = Dense(64, activation='relu', kernel_regularizer=l2(0.0001))(dropout_layer_2)\n    dropout_layer_3 = Dropout(0.5)(dense_layer_2)\n    dense_layer_3 = Dense(32, activation='relu', kernel_regularizer=l2(0.0001))(dropout_layer_3)\n    dropout_layer_4 = Dropout(0.5)(dense_layer_3)\n\n    output_layer = Dense(label_count, activation='softmax')(dropout_layer_4)\n\n    model = Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer=Adam(learning_rate=2e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Build and train DNN model\nvocab_size = len(vocab) + 1  # Account for padding index\nembedding_dim = 100\n# model = build_dnn_model(vocab_size, embedding_dim, max_len, len(label_mapping))\n\n# #Train model\n# model.fit(X_train_padded, y_train_encoded_categorical, validation_data=(X_val_padded, y_val_encoded_categorical), epochs=1, batch_size=32)\n\n# #Evaluate model\n# val_loss, val_accuracy = model.evaluate(X_val_padded, y_val_encoded_categorical)\n# print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T04:07:27.860524Z","iopub.execute_input":"2024-12-26T04:07:27.860784Z","iopub.status.idle":"2024-12-26T04:07:38.032681Z","shell.execute_reply.started":"2024-12-26T04:07:27.860761Z","shell.execute_reply":"2024-12-26T04:07:38.032019Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TF-IDF Vectorization\n","metadata":{}},{"cell_type":"code","source":"import joblib\nvectorizer = TfidfVectorizer(max_features=20000)\nX_train_tfidf = vectorizer.fit_transform(X_train).toarray()\nX_val_tfidf = vectorizer.transform(X_val).toarray()\n\njoblib.dump(vectorizer, '/kaggle/working/tfidf_vectorizer.pkl')\n\n# Convert to TensorFlow tensors\nX_train_tensor = tf.convert_to_tensor(X_train_tfidf, dtype=tf.float32)\ny_train_tensor = tf.convert_to_tensor(y_train_encoded, dtype=tf.int64)\nX_val_tensor = tf.convert_to_tensor(X_val_tfidf, dtype=tf.float32)\ny_val_tensor = tf.convert_to_tensor(y_val_encoded, dtype=tf.int64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T09:40:06.404745Z","iopub.execute_input":"2024-12-26T09:40:06.405117Z","iopub.status.idle":"2024-12-26T09:40:14.68339Z","shell.execute_reply.started":"2024-12-26T09:40:06.405089Z","shell.execute_reply":"2024-12-26T09:40:14.682657Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FFNN model\n","metadata":{}},{"cell_type":"code","source":"def build_ffnn_model(input_dim, label_count):\n    model = tf.keras.Sequential([\n        tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n        tf.keras.layers.Dense(label_count, activation='softmax')\n    ])\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# # Build and train FFNN model\n# ffnn_model = build_ffnn_model(X_train_tensor.shape[1], len(label_mapping))\n# ffnn_model.fit(X_train_tensor, y_train_tensor, validation_data=(X_val_tensor, y_val_tensor), epochs=1, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T04:07:45.431411Z","iopub.execute_input":"2024-12-26T04:07:45.431627Z","iopub.status.idle":"2024-12-26T04:07:52.384193Z","shell.execute_reply.started":"2024-12-26T04:07:45.431608Z","shell.execute_reply":"2024-12-26T04:07:52.383475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN |","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    # Input shape is inferred here\n    tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same', input_shape=(X_train_tfidf.shape[1], 1)),\n    tf.keras.layers.MaxPooling1D(pool_size=2),\n    \n    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n    tf.keras.layers.MaxPooling1D(pool_size=2),\n    \n    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n    tf.keras.layers.MaxPooling1D(pool_size=2),\n    \n    tf.keras.layers.Flatten(),\n    \n    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n    tf.keras.layers.Dropout(0.5),  # Dropout for regularization\n    \n    tf.keras.layers.Dense(len(label_mapping), activation='softmax')  # Output layer with softmax activation\n])\n\n# Compile the model\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n    metrics=['accuracy']\n)\n\n# Print the model summary (to ensure trainable parameters are not 0)\n# model.summary()\n\n# Training the model\nepochs = 12\nbatch_size = 124\n\n# with tf.device('/GPU:0'):  # Forces model training on GPU (Optional, only if GPU is detected)\n#     model.fit(\n#         X_train_tensor, y_train_tensor,\n#         validation_data=(X_val_tensor, y_val_tensor),\n#         epochs=epochs,\n#         batch_size=batch_size,\n#         shuffle=True\n#     )\n\n# # Evaluate the model on validation data\n# val_loss, val_accuracy = model.evaluate(X_val_tensor, y_val_tensor)\n# print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}%\")\n\n# model.save('/kaggle/working/CNN1.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T09:46:29.951996Z","iopub.execute_input":"2024-12-26T09:46:29.952369Z","iopub.status.idle":"2024-12-26T10:04:59.988282Z","shell.execute_reply.started":"2024-12-26T09:46:29.952346Z","shell.execute_reply":"2024-12-26T10:04:59.98713Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN ||","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Load and display the image\nimage_path = '/kaggle/input/cnn-2-archi/xLrP6IM.png'  # Replace with the path to your image\nimg = mpimg.imread(image_path)\n\n# Display the image\nplt.imshow(img)\nplt.axis('off')  # Turn off axis labels for better display\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T04:10:30.690138Z","iopub.execute_input":"2024-12-26T04:10:30.690368Z","iopub.status.idle":"2024-12-26T04:10:30.832361Z","shell.execute_reply.started":"2024-12-26T04:10:30.690347Z","shell.execute_reply":"2024-12-26T04:10:30.83069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenizer setup\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X_train)\n\nword_index = tokenizer.word_index\nX_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=1000)\nX_val_padded = pad_sequences(tokenizer.texts_to_sequences(X_val), maxlen=1000)\n\n\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nembedding_layer = Embedding(\n    input_dim=len(word_index) + 1,\n    output_dim=EMBEDDING_DIM,\n    weights=[embedding_matrix],\n    input_length=MAX_SEQUENCE_LENGTH,\n    trainable=True\n)\n\n# CNN Model with GloVe Embeddings\ndef create_cnn_model():\n    inputs = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    embedding = embedding_layer(inputs)\n    \n    reshape = tf.keras.layers.Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(embedding)\n    \n    conv_0 = tf.keras.layers.Conv2D(num_filters, kernel_size=(filter_sizes[0], EMBEDDING_DIM), activation='relu')(reshape)\n    conv_1 = tf.keras.layers.Conv2D(num_filters, kernel_size=(filter_sizes[1], EMBEDDING_DIM), activation='relu')(reshape)\n    conv_2 = tf.keras.layers.Conv2D(num_filters, kernel_size=(filter_sizes[2], EMBEDDING_DIM), activation='relu')(reshape)\n    \n    maxpool_0 = tf.keras.layers.MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1))(conv_0)\n    maxpool_1 = tf.keras.layers.MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1))(conv_1)\n    maxpool_2 = tf.keras.layers.MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1))(conv_2)\n    \n    concatenated_tensor = tf.keras.layers.Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n    flatten = tf.keras.layers.Flatten()(concatenated_tensor)\n    dropout = tf.keras.layers.Dropout(drop)(flatten)\n    output = tf.keras.layers.Dense(len(label_mapping), activation='softmax')(dropout)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    model.compile(\n        loss='sparse_categorical_crossentropy',\n        optimizer=Adam(learning_rate=1e-4),\n        metrics=['accuracy']\n    )\n    return model\n\ncnn_model = create_cnn_model()\nepochs = 1\nbatch_size = 64\n\n# cnn_model.fit(\n#     X_train_padded, y_train_encoded,\n#     validation_data=(X_val_padded, y_val_encoded),\n#     epochs=epochs,\n#     batch_size=batch_size,\n#     shuffle=True\n# )\n\n# # Evaluate the model\n# val_loss, val_accuracy = cnn_model.evaluate(X_val_padded, y_val_encoded)\n# print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T04:10:30.834644Z","iopub.execute_input":"2024-12-26T04:10:30.835117Z","iopub.status.idle":"2024-12-26T04:11:24.136293Z","shell.execute_reply.started":"2024-12-26T04:10:30.835067Z","shell.execute_reply":"2024-12-26T04:11:24.135103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"# LSTM Classifier Model\nclass ImprovedLSTMClassifier(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, output_dim, max_length=100):\n        super(ImprovedLSTMClassifier, self).__init__()\n        \n        # Embedding layer with spatial dropout\n        self.embedding = tf.keras.layers.Embedding(\n            vocab_size, \n            embedding_dim,\n            input_length=max_length,\n            embeddings_regularizer=tf.keras.regularizers.l2(1e-5)\n        )\n        self.spatial_dropout = tf.keras.layers.SpatialDropout1D(0.2)\n        \n        # First Bidirectional LSTM layer with higher units\n        self.lstm1 = tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(\n                256,\n                return_sequences=True,\n                dropout=0.3,\n                recurrent_dropout=0.3,\n                kernel_regularizer=tf.keras.regularizers.l2(1e-5)\n            )\n        )\n        \n        # Second Bidirectional LSTM layer\n        self.lstm2 = tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(\n                128,\n                return_sequences=False,\n                dropout=0.3,\n                recurrent_dropout=0.3,\n                kernel_regularizer=tf.keras.regularizers.l2(1e-5)\n            )\n        )\n        \n        # Dense layers with batch normalization and dropout\n        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n        self.dropout1 = tf.keras.layers.Dropout(0.4)\n        \n        self.dense1 = tf.keras.layers.Dense(\n            512, \n            activation='relu',\n            kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n            kernel_initializer='he_normal'\n        )\n        \n        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n        self.dropout2 = tf.keras.layers.Dropout(0.4)\n        \n        self.dense2 = tf.keras.layers.Dense(\n            256, \n            activation='relu',\n            kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n            kernel_initializer='he_normal'\n        )\n        \n        # Output layer\n        self.output_layer = tf.keras.layers.Dense(\n            output_dim, \n            activation='softmax',\n            kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n        )\n\n    def call(self, inputs, training=False):\n        # Embedding with spatial dropout\n        x = self.embedding(inputs)\n        x = self.spatial_dropout(x, training=training)\n        \n        # LSTM layers\n        x = self.lstm1(x, training=training)\n        x = self.lstm2(x, training=training)\n        \n        # First dense block\n        x = self.batch_norm1(x, training=training)\n        x = self.dropout1(x, training=training)\n        x = self.dense1(x)\n        \n        # Second dense block\n        x = self.batch_norm2(x, training=training)\n        x = self.dropout2(x, training=training)\n        x = self.dense2(x)\n        \n        # Output\n        return self.output_layer(x)\n\nvocab_size = len(tokenizer.word_index) + 1\nembedding_dim = 200\nnum_classes =5\n# Initialize distributed strategy\nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n    model = ImprovedLSTMClassifier(vocab_size, embedding_dim, num_classes)\n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        metrics=['accuracy']\n    )\n\n# Train model directly with numpy arrays\n# epochs = 1\n# batch_size = 256\n# model.fit(\n#     X_train_padded, \n#     y_train_encoded_categorical,\n#     batch_size=batch_size,\n#     epochs=epochs,\n#     validation_data=(X_val_padded, y_val_encoded_categorical),\n#     shuffle=True\n# )\n\n# # Evaluate\n# val_loss, val_accuracy = model.evaluate(\n#     X_val_padded, \n#     y_val_encoded_categorical, \n#     batch_size=batch_size\n# )\n# print(f\"Validation Loss: {val_loss:.4f}\")\n# print(f\"Validation Accuracy: {val_accuracy*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T04:22:25.873056Z","iopub.execute_input":"2024-12-26T04:22:25.873423Z","iopub.status.idle":"2024-12-26T04:22:55.199368Z","shell.execute_reply.started":"2024-12-26T04:22:25.873392Z","shell.execute_reply":"2024-12-26T04:22:55.198537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RNN","metadata":{}},{"cell_type":"code","source":"# Function to create the RNN model\ndef create_rnn_model(vocab_size, embedding_dim, output_dim):\n    inputs = tf.keras.Input(shape=(None,))  # Input layer for sequences\n    embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)  # Embedding layer\n    # Bidirectional RNN layer\n    rnn = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(32, return_sequences=True, dropout=0.3))(embedding)  # Bidirectional RNN\n    # Batch normalization\n    batch_norm = tf.keras.layers.BatchNormalization()(rnn)\n    # Get the last hidden state\n    last_hidden_state = batch_norm[:, -1, :]  # Use the last hidden state\n    # Fully connected layer with L2 regularization\n    fc1 = tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(last_hidden_state)    \n    # Dropout layer\n    dropout = tf.keras.layers.Dropout(0.5)(fc1)\n    # Final output layer with softmax activation\n    output = tf.keras.layers.Dense(output_dim, activation='softmax')(dropout)\n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    return model\n\n# Hyperparameters\nvocab_size = len(vocab) + 1  # +1 for padding token\nembedding_dim = 300  # Increase embedding dimension\noutput_dim = len(label_mapping)  # Number of categories (5 in this case)\n\n# Create and compile the model inside the strategy scope for multi-GPU usage (optional)\nstrategy = tf.distribute.MirroredStrategy()  # Use multiple GPUs if available\nprint(f'Number of devices: {strategy.num_replicas_in_sync}')\n\nwith strategy.scope():\n    model = create_rnn_model(vocab_size, embedding_dim, output_dim)\n\n    # Compile the model\n    model.compile(\n        loss='categorical_crossentropy',  # Use categorical crossentropy for one-hot encoded labels\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        metrics=['accuracy']\n    )\n\n# Training the model\n# epochs = 1\n\n# #Train the model using tf.data API for batched data\n# model.fit(\n#     X_train_padded, \n#     y_train_encoded_categorical,\n#     batch_size=batch_size,\n#     epochs=epochs,\n#     validation_data=(X_val_padded, y_val_encoded_categorical),\n# )\n# # Evaluate the model on validation data\n# val_loss, val_accuracy = model.evaluate(X_val_padded, y_val_encoded_categorical)\n# print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T04:24:15.720869Z","iopub.execute_input":"2024-12-26T04:24:15.721261Z","iopub.status.idle":"2024-12-26T04:27:30.771793Z","shell.execute_reply.started":"2024-12-26T04:24:15.721232Z","shell.execute_reply":"2024-12-26T04:27:30.770685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T04:28:42.422334Z","iopub.execute_input":"2024-12-26T04:28:42.422738Z","iopub.status.idle":"2024-12-26T04:30:10.265662Z","shell.execute_reply.started":"2024-12-26T04:28:42.422683Z","shell.execute_reply":"2024-12-26T04:30:10.264944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transformer |","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport nltk\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers, models\nimport os\n\n# Define the transformer encoder block as a function\ndef transformer_encoder(inputs, embed_dim, num_heads, ff_dim, dropout_rate=0.1, training=False):\n    # Multi-Head Self-Attention\n    attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n    attention = layers.Dropout(dropout_rate)(attention, training=training)  # Apply dropout if in training mode\n    attention_output = layers.LayerNormalization(epsilon=1e-6)(inputs + attention)  # Add & Normalize\n    # Feed-forward network\n    ff_output = layers.Dense(ff_dim, activation='relu')(attention_output)\n    ff_output = layers.Dense(embed_dim)(ff_output)\n    ff_output = layers.Dropout(dropout_rate)(ff_output, training=training)  # Apply dropout if in training mode\n    output = layers.LayerNormalization(epsilon=1e-6)(attention_output + ff_output)  # Add & Normalize\n    \n    return output\n\n# Transformer Model\ndef transformer_model(vocab_size, embed_dim, num_heads, ff_dim, num_blocks, max_len, num_classes, dropout_rate=0.1):\n    # Input layer: (batch_size, sequence_length)\n    inputs = layers.Input(shape=(max_len,))  # Shape: (None, max_len)\n    \n    # Embedding layer\n    embedding = layers.Embedding(vocab_size, embed_dim)(inputs)  # Shape: (None, max_len, embed_dim)\n    \n    # Positional Encoding (it should be of shape (max_len, embed_dim))\n    positional_encoding = layers.Embedding(input_dim=max_len, output_dim=embed_dim)(tf.range(max_len))  # Shape: (max_len, embed_dim)\n    \n    # Add positional encoding to embedding\n    x = embedding + positional_encoding  # Shape: (None, max_len, embed_dim)\n    \n    # Dropout after embedding + positional encoding\n    x = layers.Dropout(dropout_rate)(x)\n    \n    # Transformer blocks\n    for _ in range(num_blocks):\n        x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout_rate)\n    \n    # Global Average Pooling\n    x = layers.GlobalAveragePooling1D()(x)  # Shape: (None, embed_dim)\n    \n    # Output layer\n    outputs = layers.Dense(num_classes, activation='softmax')(x)  # Shape: (None, num_classes)\n    \n    # Create the model\n    model = models.Model(inputs=inputs, outputs=outputs)\n    \n    return model\n\nembed_dim = 64  # Embedding dimension\nnum_heads = 2  # Number of attention heads\nff_dim = 64  # Feed-forward layer dimension\nnum_blocks = 2  # Number of transformer blocks\nmax_len = 1000  # Max sequence length (fixed to match input data)\nvocab_size = len(vocab) + 1  # Add padding index\nnum_classes = len(label_mapping)  # Number of output classes\ndropout_rate = 0.1  # Dropout rate\nbatch_size=16\nepochs = 1\n\n# Create the model using the transformer function\nmodel = transformer_model(vocab_size, embed_dim, num_heads, ff_dim, num_blocks, max_len, num_classes)\n\n# Compile the model\nmodel.compile(\n    loss='categorical_crossentropy',  # For multi-class classification\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n    metrics=['accuracy']\n)\n\n\n\n# model.fit(\n#     X_train_padded, \n#     y_train_encoded_categorical,\n#     batch_size=batch_size,\n#     epochs=epochs,\n#     validation_data=(X_val_padded, y_val_encoded_categorical),\n#     shuffle=True\n# )\n# # Evaluate the model on validation data\n# val_loss, val_accuracy = model.evaluate(val_dataset)\n# print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T04:34:59.824597Z","iopub.execute_input":"2024-12-26T04:34:59.824975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transformer ||","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFRobertaForSequenceClassification, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport re\nimport emoji\nimport unicodedata\n\n# Ensure GPU usage with memory growth enabled before any TensorFlow operation\nphysical_devices = tf.config.list_physical_devices('GPU')\nif physical_devices:\n    print(\"Using GPU\")\n    try:\n        # Enabling memory growth on the first GPU device\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    except RuntimeError as e:\n        # Memory growth needs to be set before TensorFlow runtime initialization\n        print(f\"Error setting memory growth: {e}\")\nelse:\n    print(\"No GPU detected, using CPU\")\n\n# Set batch size and max_length variables\nbatch_size = 32  # You can change this value easily\nmax_length = 100  # You can change this value easily\n\n# Extract texts and labels (replace this with actual loading code)\ntrain_data = pd.read_csv('/kaggle/input/neural-network-dataset/Forum Discussion Categorization/train.csv')\nX = train_data['Discussion'].fillna('').astype(str)\ny = train_data['Category']\n\n# Contraction mapping for text preprocessing\ncontraction_mapping = {\n    \"won't\": \"will not\", \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\", \n    \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\", \n    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n    \"he's\": \"he is\", \"she's\": \"she is\", \"it's\": \"it is\", \"that's\": \"that is\"\n}\n\n# Preprocessing function\ndef clean_text(text):\n    # Convert to ASCII and remove non-alphanumeric characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n    # Expand contractions\n    for word, replacement in contraction_mapping.items():\n        text = text.replace(word, replacement)\n    # Remove URLs, mentions, and special characters\n    text = re.sub(r'http\\S+|www\\S+|@\\S+', '', text)\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    text = emoji.demojize(text)  # Handle emojis\n    return text.strip()\n\n# Apply preprocessing\nX_cleaned = X.apply(clean_text)\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_cleaned, y, test_size=0.2, random_state=42)\n\n# Label mapping\nlabel_mapping = {\n    'Politics': 0,\n    'Sports': 1,\n    'Media': 2,\n    'Market & Economy': 3,\n    'STEM': 4\n}\ny_train_encoded = y_train.map(label_mapping)\ny_val_encoded = y_val.map(label_mapping)\n\n# Initialize RoBERTa tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# Tokenize input text for RoBERTa (input IDs and attention masks)\ntrain_inputs = tokenizer(\n    X_train.tolist(),\n    padding=True,\n    truncation=True,\n    max_length=max_length,\n    return_tensors=\"tf\"\n)\n\nval_inputs = tokenizer(\n    X_val.tolist(),\n    padding=True,\n    truncation=True,\n    max_length=max_length,\n    return_tensors=\"tf\"\n)\n\n# Extract input_ids and attention_mask for training and validation\nX_train_ids = train_inputs['input_ids']\ntrain_attention_mask = train_inputs['attention_mask']\n\nX_val_ids = val_inputs['input_ids']\nval_attention_mask = val_inputs['attention_mask']\n\n# Convert labels to tensors\ny_train_encoded_tensor = tf.convert_to_tensor(y_train_encoded.tolist(), dtype=tf.int32)\ny_val_encoded_tensor = tf.convert_to_tensor(y_val_encoded.tolist(), dtype=tf.int32)\n\n# Create TensorFlow datasets with the defined batch size\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": X_train_ids, \"attention_mask\": train_attention_mask}, y_train_encoded_tensor\n)).batch(batch_size).shuffle(buffer_size=1024)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": X_val_ids, \"attention_mask\": val_attention_mask}, y_val_encoded_tensor\n)).batch(batch_size)\n\n# Load pre-trained RoBERTa model for embeddings (without classification head)\nroberta_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n\n# Build the custom model\ninput_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\nattention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n\n# Use the pre-trained RoBERTa model as a layer within a Keras model\nroberta_output = roberta_model.roberta(input_ids, attention_mask=attention_mask)\ncls_token = roberta_output.last_hidden_state  # Shape: (batch_size, seq_len, hidden_dim)\n\n# Custom Transformer Layer (optional, if you want to add custom layers after RoBERTa)\ndef transformer_encoder(inputs, head_size, num_heads, ff_dim):\n    attention_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n    attention_output = tf.keras.layers.LayerNormalization()(attention_output + inputs)\n\n    ff_output = tf.keras.layers.Dense(ff_dim, activation='relu')(attention_output)\n    ff_output = tf.keras.layers.Dense(inputs.shape[-1])(ff_output)\n    encoder_output = tf.keras.layers.LayerNormalization()(ff_output + attention_output)\n    \n    return encoder_output\n\n# Pass through custom transformer layers\ntransformer_output = transformer_encoder(cls_token, head_size=128, num_heads=5, ff_dim=128)\n\n# Global average pooling over the sequence length (using the first token, cls_token)\npooling_output = tf.keras.layers.GlobalAveragePooling1D()(transformer_output)\n\n# Output layer: classification using softmax\nfinal_output = tf.keras.layers.Dense(len(label_mapping), activation='softmax')(pooling_output)\n\n# Final model\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=final_output)\n\n# Compile the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-6)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\nmetrics = [tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n\n# model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n# # Train the model\n# epochs = 5\n# history = model.fit(\n#     train_dataset,\n#     validation_data=val_dataset,\n#     epochs=epochs\n# )\n\n# # Evaluate the model\n# val_loss, val_accuracy = model.evaluate(val_dataset)\n# print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n\n# # Save the model\n# model.save('/kaggle/working/Transformer2.h5')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:21:56.119848Z","iopub.execute_input":"2024-12-26T10:21:56.120085Z","iopub.status.idle":"2024-12-26T10:30:15.560424Z","shell.execute_reply.started":"2024-12-26T10:21:56.120058Z","shell.execute_reply":"2024-12-26T10:30:15.558962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test preprocessing\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport pandas as pd\nimport spacy\nimport re\nimport logging  # Import the logging module\nimport nltk\nimport os\nimport unicodedata\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport emoji\nfrom bs4 import BeautifulSoup\nimport string\nimport concurrent.futures\n\n# Configure logging\nlogging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Load SpaCy English model with word vectors\nnlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])  # Disable parser and ner for speed\n\n# Download NLTK resources\nnltk.download('stopwords', quiet=True)\nnltk.download('punkt', quiet=True)\n\n# Initialize stopwords\nstop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer(\"english\")\n\ncontraction_mapping = {\n    \"won't\": \"will not\", \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\", \n    \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\", \n    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n    \"he's\": \"he is\", \"she's\": \"she is\", \"it's\": \"it is\", \"that's\": \"that is\"\n}\n\ndef normalize_elongations(text):\n    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n\ndef convert_to_ascii(input_str):\n    nfkd_form = unicodedata.normalize('NFKD', input_str)\n    return ''.join([c for c in nfkd_form if unicodedata.category(c) != 'Mn']).lower()\n\ndef clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    for word in mapping.keys():\n        if word in text:\n            text = text.replace(word, mapping[word])\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n    text = re.sub(r'[\" \"]+', \" \", text)\n    return text\n\ndef clean_text(text):\n    if text is None:\n        return \"\"\n    if isinstance(text, str):\n        try:\n            text = text.encode('latin1').decode('utf-8')\n        except (UnicodeEncodeError, UnicodeDecodeError):\n            logging.warning(f\"Unicode error during decoding of: {text}\")\n            pass\n        text = emoji.demojize(text)\n        text = BeautifulSoup(text, 'lxml').get_text()\n        text = re.sub('<.*?>+', '', text)\n        text = re.sub('\\n', '', text)\n        text = re.sub('\\w*\\d\\w*', '', text)\n        text = convert_to_ascii(text)\n        text = normalize_elongations(text)\n        text = clean_contractions(text, contraction_mapping)\n        text = re.sub(r'http\\S+|www\\S+|@\\S+|[^a-zA-Z\\'\\-\\s]', '', text)  # keep apostrophe and dash\n        doc = nlp(text)\n        text = [stemmer.stem(token.text) for token in doc if token.text not in stop_words and not token.is_punct]\n        text = ' '.join(text).strip()\n        text = re.sub(r'\\s+', ' ', text)\n        return text\n    else:\n        return \"\"\n\n# Load the test data\ntest_data = pd.read_csv('/kaggle/input/disc-dataset/ThursdayTest.csv')\nX_test = test_data['Discussion'].fillna('').astype(str)\n\n# Clean the test data\ndef parallel_clean_text(X_data, num_workers=os.cpu_count()):  # Adjust workers\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        cleaned_text = list(executor.map(clean_text, X_data))\n    return cleaned_text\n\nX_test_cleaned = parallel_clean_text(X_test)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T09:43:43.132005Z","iopub.execute_input":"2024-12-26T09:43:43.132419Z","iopub.status.idle":"2024-12-26T09:43:45.169462Z","shell.execute_reply.started":"2024-12-26T09:43:43.132385Z","shell.execute_reply":"2024-12-26T09:43:45.168578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\nvectorizer = joblib.load('/kaggle/working/tfidf_vectorizer.pkl')\n\nX_test_tfidf = vectorizer.transform(X_test_cleaned).toarray()  # Transform the test data\n\n\nX_test_tensor = tf.convert_to_tensor(X_test_tfidf, dtype=tf.float32)\n\n# If you have a trained model, you can now predict on the test data\n# model_FNN = tf.keras.models.load_model('FNN.h5')\n# predictions = model_FNN.predict(X_test_tensor)\n\nmodel_CNN1 = tf.keras.models.load_model('CNN1.h5')\npredictions = model_CNN1.predict(X_test_tensor)\n\n\npredicted_classes = tf.argmax(predictions, axis=1).numpy()\n\n# Prepare the submission file\nsubmission = pd.DataFrame({\n    'SampleID': test_data['SampleID'],  # Assuming 'SampleID' is a column in the test data\n    'Category': predicted_classes\n})\n\n# Save the predictions to a CSV file\nsubmission.to_csv('/kaggle/working/CS_22 submission.csv', index=False)\n\nprint(\"Submission file generated successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T09:43:45.170723Z","iopub.execute_input":"2024-12-26T09:43:45.171078Z","iopub.status.idle":"2024-12-26T09:43:46.750771Z","shell.execute_reply.started":"2024-12-26T09:43:45.171039Z","shell.execute_reply":"2024-12-26T09:43:46.750022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the test data (replace with your actual test data path)\ntest_data = pd.read_csv('/kaggle/input/disc-dataset/ThursdayTest.csv')\nX_test = test_data['Discussion'].fillna('').astype(str)\n\n# Apply preprocessing to the test data\nX_test_cleaned = X_test.apply(clean_text)\n\n# Tokenize the test data using the same tokenizer\ntest_inputs = tokenizer(\n    X_test_cleaned.tolist(),\n    padding=True,\n    truncation=True,\n    max_length=max_length,\n    return_tensors=\"tf\"\n)\n\n# Extract input_ids and attention_mask for the test data\nX_test_ids = test_inputs['input_ids']\ntest_attention_mask = test_inputs['attention_mask']\n\n# Create the test dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": X_test_ids, \"attention_mask\": test_attention_mask}\n)).batch(batch_size)\n\n# Make predictions on the test set\nmodel_Trans = tf.keras.models.load_model('/kaggle/working/Transformer2.h5')\npredictions = model_Trans.predict(X_test_tensor)\n# Convert predictions to class labels\npredicted_classes = tf.argmax(predictions.logits, axis=1).numpy()\n\n# Prepare the submission file (assuming 'SampleID' exists in the test data)\nsubmission = pd.DataFrame({\n    'SampleID': test_data['SampleID'],  # Replace with the actual column name if it's different\n    'Category': predicted_classes\n})\n\n# Save the predictions to a CSV file (you can change the path as needed)\nsubmission.to_csv('/kaggle/working/CS_22_submission.csv', index=False)\n\nprint(\"Submission file generated successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFBertForSequenceClassification, BertTokenizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport re\nimport emoji\nimport unicodedata\n\n# Ensure GPU usage with memory growth enabled before any TensorFlow operation\nphysical_devices = tf.config.list_physical_devices('GPU')\nif physical_devices:\n    print(\"Using GPU\")\n    try:\n        # Enabling memory growth on the first GPU device\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    except RuntimeError as e:\n        # Memory growth needs to be set before TensorFlow runtime initialization\n        print(f\"Error setting memory growth: {e}\")\nelse:\n    print(\"No GPU detected, using CPU\")\n\n# Set batch size and max_length variables\nbatch_size = 32  # You can change this value easily\nmax_length = 256  # You can change this value easily\n\n\n# Extract texts and labels (replace this with actual loading code)\ntrain_data = pd.read_csv('/kaggle/input/neural-network-dataset/Forum Discussion Categorization/train.csv')\nX = train_data['Discussion'].fillna('').astype(str)\ny = train_data['Category']\n\n# Contraction mapping for text preprocessing\ncontraction_mapping = {\n    \"won't\": \"will not\", \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\", \n    \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\", \n    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n    \"he's\": \"he is\", \"she's\": \"she is\", \"it's\": \"it is\", \"that's\": \"that is\"\n}\n\n# Preprocessing function\ndef clean_text(text):\n    # Convert to ASCII and remove non-alphanumeric characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n    # Expand contractions\n    for word, replacement in contraction_mapping.items():\n        text = text.replace(word, replacement)\n    # Remove URLs, mentions, and special characters\n    text = re.sub(r'http\\S+|www\\S+|@\\S+', '', text)\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    text = emoji.demojize(text)  # Handle emojis\n    return text.strip()\n\n# Apply preprocessing\nX_cleaned = X.apply(clean_text)\n\n\nprint(\"done!\")\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_cleaned, y, test_size=0.2, random_state=42)\n\n# Label mapping\nlabel_mapping = {\n    'Politics': 0,\n    'Sports': 1,\n    'Media': 2,\n    'Market & Economy': 3,\n    'STEM': 4\n}\ny_train_encoded = y_train.map(label_mapping)\ny_val_encoded = y_val.map(label_mapping)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:30:29.152919Z","iopub.execute_input":"2024-12-26T10:30:29.153234Z","iopub.status.idle":"2024-12-26T10:30:35.106185Z","shell.execute_reply.started":"2024-12-26T10:30:29.153206Z","shell.execute_reply":"2024-12-26T10:30:35.105147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize input text for BERT (input IDs and attention masks)\ntrain_inputs = tokenizer(\n    X_train.tolist(),\n    padding=True,\n    truncation=True,\n    max_length=max_length,\n    return_tensors=\"tf\"\n)\n\nval_inputs = tokenizer(\n    X_val.tolist(),\n    padding=True,\n    truncation=True,\n    max_length=max_length,\n    return_tensors=\"tf\"\n)\n\n# Extract input_ids and attention_mask for training and validation\nX_train_ids = train_inputs['input_ids']\ntrain_attention_mask = train_inputs['attention_mask']\n\nX_val_ids = val_inputs['input_ids']\nval_attention_mask = val_inputs['attention_mask']\n\n# Convert labels to tensors\ny_train_encoded_tensor = tf.convert_to_tensor(y_train_encoded.tolist(), dtype=tf.int32)\ny_val_encoded_tensor = tf.convert_to_tensor(y_val_encoded.tolist(), dtype=tf.int32)\n\n# Create TensorFlow datasets with the defined batch size\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": X_train_ids, \"attention_mask\": train_attention_mask}, y_train_encoded_tensor\n)).batch(batch_size).shuffle(buffer_size=1024)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {\"input_ids\": X_val_ids, \"attention_mask\": val_attention_mask}, y_val_encoded_tensor\n)).batch(batch_size)\n\n# Load pre-trained BERT model for embeddings (without classification head)\nbert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n\n# Extract the transformer layers (without the classification head)\nbert_transformer = bert_model.bert\n\n# Custom Transformer Layer (optional, if you want to add custom layers after BERT)\ndef transformer_encoder(inputs, head_size, num_heads, ff_dim):\n    attention_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n    attention_output = tf.keras.layers.LayerNormalization()(attention_output + inputs)\n\n    ff_output = tf.keras.layers.Dense(ff_dim, activation='relu')(attention_output)\n    ff_output = tf.keras.layers.Dense(inputs.shape[-1])(ff_output)\n    encoder_output = tf.keras.layers.LayerNormalization()(ff_output + attention_output)\n    \n    return encoder_output\n\n# Build the custom model\ninput_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\nattention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n\n# Use the pre-trained BERT model to get embeddings\nbert_output = bert_transformer(input_ids, attention_mask=attention_mask)\ncls_token = bert_output.last_hidden_state  # Shape: (batch_size, seq_len, hidden_dim)\n\n# Pass through custom transformer layers\ntransformer_output = transformer_encoder(cls_token, head_size=128, num_heads=3, ff_dim=128)\n\n# Global average pooling over the sequence length (using the first token, cls_token)\npooling_output = tf.keras.layers.GlobalAveragePooling1D()(transformer_output)\n\n# Output layer: classification using softmax\nfinal_output = tf.keras.layers.Dense(len(label_mapping), activation='softmax')(pooling_output)\n\n# Final model\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=final_output)\n\n# Compile the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-6)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\nmetrics = [tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n# Train the model\nepochs = 5\nhistory = model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=epochs\n)\n\n# Evaluate the model\nval_loss, val_accuracy = model.evaluate(val_dataset)\nprint(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:30:35.107252Z","iopub.execute_input":"2024-12-26T10:30:35.10751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save('/kaggle/working/Transformer2.h5')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}